# ICON4Py - ICON inspired code in Python and GT4Py
#
# Copyright (c) 2022-2024, ETH Zurich and MeteoSwiss
# All rights reserved.
#
# Please, refer to the LICENSE file in the root directory.
# SPDX-License-Identifier: BSD-3-Clause

import sys
import os
import shutil
import math
import zipfile
import click
import humanize
import numpy as np
import dask
import pandas as pd
import xarray as xr
import zarr
from zarr_any_numcodecs import AnyNumcodecsArrayArrayCodec, AnyNumcodecsArrayBytesCodec, AnyNumcodecsBytesBytesCodec
import numcodecs
import numcodecs.zarr3
import zfpy
from ebcc.filter_wrapper import EBCC_Filter
from ebcc.zarr_filter import EBCCZarrFilter
from mpi4py import MPI
import time
from collections import defaultdict
import atexit

# numcodecs-wasm filters
from numcodecs_wasm_asinh import Asinh
from numcodecs_wasm_bit_round import BitRound
from numcodecs_wasm_fixed_offset_scale import FixedOffsetScale
from numcodecs_wasm_identity import Identity
from numcodecs_wasm_linear_quantize import LinearQuantize
from numcodecs_wasm_log import Log
from numcodecs_wasm_round import Round
from numcodecs_wasm_swizzle_reshape import SwizzleReshape
from numcodecs_wasm_uniform_noise import UniformNoise
# numcodecs-wasm compressors
from numcodecs_wasm_zlib import Zlib
from numcodecs_wasm_zstd import Zstd
# numcodecs-wasm serializers
from numcodecs_wasm_pco import Pco
from numcodecs_wasm_sperr import Sperr
from numcodecs_wasm_sz3 import Sz3
from numcodecs_wasm_zfp import Zfp


_WITH_NUMCODECS_WASM = os.getenv("WITH_NUMCODECS_WASM", "true").lower() in ("1", "true", "yes")
_WITH_EBCC = os.getenv("WITH_EBCC", "true").lower() in ("1", "true", "yes")


def open_netcdf(netcdf_file: str, field_to_compress: str | None = None, field_percentage_to_compress: float | None = None, rank: int = 0):
    ds = xr.open_dataset(netcdf_file, chunks="auto")  # auto for Dask backend

    if field_to_compress is not None and field_to_compress not in ds.data_vars:
        if rank == 0:
            click.echo(f"Field {field_to_compress} not found in NetCDF file.")
            click.echo(f"Available fields in the dataset: {list(ds.data_vars.keys())}.")
            click.echo("Aborting...")
        sys.exit(1)

    if rank == 0:
        click.echo(f"netcdf_file.nbytes = {humanize.naturalsize(ds.nbytes, binary=True)}")
        if field_to_compress is not None:
            nbytes = ds[field_to_compress].nbytes * (field_percentage_to_compress / 100) if field_percentage_to_compress else ds[field_to_compress].nbytes
            click.echo(f"field_to_compress.nbytes = {humanize.naturalsize(nbytes, binary=True)}")

    return ds


def open_zarr_zipstore(zarr_zipstore_file: str):
    store = zarr.storage.ZipStore(zarr_zipstore_file, read_only=True)
    return zarr.open_group(store, mode='r'), store


def compress_with_zarr(data, netcdf_file, field_to_compress, where_to_write, filters, compressors, serializer, verbose=True, rank=0):
    assert isinstance(data.data, dask.array.Array)

    basename = os.path.basename(netcdf_file)
    zarr_file = os.path.join(where_to_write, basename)
    zarr_file = f"{zarr_file}.=.field_{field_to_compress}.=.rank_{rank}.zarr.zip"

    with Timer("zarr.create_array"):
        store = zarr.storage.ZipStore(zarr_file, mode='w')
        zarr.create_array(
            store=store,
            name=field_to_compress,
            data=data,
            chunks="auto",
            filters=filters,
            compressors=compressors,
            serializer=serializer,
            )
        store.close()

    group, store = open_zarr_zipstore(zarr_file)
    z = group[field_to_compress]
    z_dask = dask.array.from_zarr(z)

    info_array = z.info_complete()
    compression_ratio = info_array._count_bytes / info_array._count_bytes_stored
    if verbose and rank == 0:
        click.echo(80* "-")
        click.echo(info_array)

    with Timer("compute_errors_distances"):
        pprint_, errors, euclidean_distance, normalized_euclidean_distance = compute_errors_distances(z_dask, data.data)
    if verbose and rank == 0:
        click.echo(80* "-")
        click.echo(pprint_)
        click.echo(80* "-")
        click.echo(f"Euclidean Distance: {euclidean_distance}")
        click.echo(80* "-")

    store.close()

    return compression_ratio, errors, euclidean_distance


def compute_errors_distances(da_compressed, da):
    da_error = da_compressed - da

    # These are still lazy Dask computations
    norm_L1_error = np.abs(da_error).sum()
    norm_L2_error = np.sqrt((da_error**2).sum())
    norm_Linf_error = np.abs(da_error).max()

    norm_L1_original = np.abs(da).sum()
    norm_L2_original = np.sqrt((da**2).sum())
    norm_Linf_original = np.abs(da).max()

    # Group all into one call to compute for efficiency
    computed = dask.compute(
        norm_L1_error,
        norm_L1_original,
        norm_L2_error,
        norm_L2_original,
        norm_Linf_error,
        norm_Linf_original,
    )

    (
        norm_L1_error_val,
        norm_L1_original_val,
        norm_L2_error_val,
        norm_L2_original_val,
        norm_Linf_error_val,
        norm_Linf_original_val,
    ) = computed

    relative_error_L1 = norm_L1_error_val / norm_L1_original_val
    relative_error_L2 = norm_L2_error_val / norm_L2_original_val
    relative_error_Linf = norm_Linf_error_val / norm_Linf_original_val

    euclidean_distance = norm_L2_error_val
    normalized_euclidean_distance = relative_error_L2

    errors = {
        "Relative_Error_L1": relative_error_L1,
        "Relative_Error_L2": relative_error_L2,
        "Relative_Error_Linf": relative_error_Linf,
    }

    errors_ = {k: f"{v:.3e}" for k, v in errors.items()}
    return "\n".join(f"{k:20s}: {v}" for k, v in errors_.items()), errors, euclidean_distance, normalized_euclidean_distance


def compressor_space(da, compressor_class):
    # https://numcodecs.readthedocs.io/en/stable/zarr3.html#compressors-bytes-to-bytes-codecs

    compressor_space = []

    _COMPRESSORS = [numcodecs.zarr3.Blosc, numcodecs.zarr3.LZ4, numcodecs.zarr3.Zstd, numcodecs.zarr3.Zlib, numcodecs.zarr3.GZip, numcodecs.zarr3.BZ2, numcodecs.zarr3.LZMA]
    _COMPRESSOR_MAP = {cls.__name__.lower(): cls for cls in _COMPRESSORS}

    if compressor_class and compressor_class.lower() in _COMPRESSOR_MAP:
        _COMPRESSORS = [_COMPRESSOR_MAP[compressor_class.lower()]]
    elif compressor_class and compressor_class.lower() == "none":
        _COMPRESSORS = []
        compressor_space.append(None)

    for compressor in _COMPRESSORS:
        if compressor == numcodecs.zarr3.Blosc:
            for cname in numcodecs.blosc.list_compressors():
                for clevel in [1, 6, 9]:
                    for shuffle in [-1, 0, 1, 2]:
                        compressor_space.append(compressor(cname=cname, clevel=clevel, shuffle=shuffle))
        elif compressor == numcodecs.zarr3.LZ4:
            for acceleration in [1, 10, 100]:
                compressor_space.append(compressor(acceleration=acceleration))
        elif compressor == numcodecs.zarr3.Zstd:
            for level in [0, 1, 9, 22]:
                compressor_space.append(compressor(level=level))
        elif compressor == numcodecs.zarr3.Zlib:
            for level in [1, 6, 9]:
                compressor_space.append(compressor(level=level))
        elif compressor == numcodecs.zarr3.GZip:
            for level in [1, 6, 9]:
                compressor_space.append(compressor(level=level))
        elif compressor == numcodecs.zarr3.BZ2:
            for level in [1, 6, 9]:
                compressor_space.append(compressor(level=level))
        elif compressor == numcodecs.zarr3.LZMA:
            for preset in [1, 6, 9]:
                compressor_space.append(compressor(preset=preset))

    return list(zip(range(len(compressor_space)), compressor_space))


def filter_space(da, filter_class):
    # https://numcodecs.readthedocs.io/en/stable/zarr3.html#filters-array-to-array-codecs
    # https://numcodecs-wasm.readthedocs.io/en/latest/

    filter_space = []

    _FILTERS = [numcodecs.zarr3.Delta, numcodecs.zarr3.BitRound, numcodecs.zarr3.Quantize]
    if _WITH_NUMCODECS_WASM:
        _FILTERS += [Asinh, FixedOffsetScale]
    if da.dtype.kind == 'i':
        _FILTERS = [numcodecs.zarr3.Delta]

    _FILTER_MAP = {cls.__name__.lower(): cls for cls in _FILTERS}
    if filter_class and filter_class.lower() in _FILTER_MAP:
        _FILTERS = [_FILTER_MAP[filter_class.lower()]]
    elif filter_class and filter_class.lower() == "none":
        _FILTERS = []
        filter_space.append(None)

    for filt in _FILTERS:
        if filt == numcodecs.zarr3.Delta:
            if np.issubdtype(da.dtype, np.number):
                filter_space.append(filt(dtype=str(da.dtype)))
        elif filt == numcodecs.zarr3.BitRound:
            for keepbits in valid_keepbits_for_bitround(da, step=9):
                filter_space.append(filt(keepbits=keepbits))
        elif filt == numcodecs.zarr3.Quantize:
            for digits in valid_digits_for_quantize(da, step=4):
                filter_space.append(filt(digits=digits, dtype=str(da.dtype)))
        elif filt == Asinh:
            filter_space.append(AnyNumcodecsArrayArrayCodec(filt(linear_width=compute_linear_width(da, quantile=0.01, compute=True))))
        elif filt == FixedOffsetScale:
            # Compute required stats ONCE (global reductions; memory-light but do trigger a compute)
            mean_val, std_val, min_val, max_val = dask.compute(
                da.mean(skipna=True),
                da.std(skipna=True),
                da.min(skipna=True),
                da.max(skipna=True),
            )

            # Helper to validate finite, nonzero scale (avoid divide-by-zero / NaNs)
            def _safe_scale(x, min_eps=1e-12):
                if not np.isfinite(x):
                    return None
                if abs(x) < min_eps:
                    return None
                return float(x)

            # Normalize: (x - mean) / std
            std_safe = _safe_scale(std_val)
            if np.isfinite(mean_val) and std_safe is not None:
                filter_space.append(AnyNumcodecsArrayArrayCodec(filt(offset=float(mean_val), scale=std_safe)))

            # Standardize to [0,1]-like: (x - min) / (max - min)
            rng = max_val - min_val
            rng_safe = _safe_scale(rng)
            if np.isfinite(min_val) and rng_safe is not None:
                filter_space.append(AnyNumcodecsArrayArrayCodec(filt(offset=float(min_val), scale=rng_safe)))

    return list(zip(range(len(filter_space)), filter_space))


def serializer_space(da, serializer_class):
    # https://numcodecs.readthedocs.io/en/stable/zarr3.html#serializers-array-to-bytes-codecs
    # https://numcodecs-wasm.readthedocs.io/en/latest/
    is_int = (da.dtype.kind == "i")

    serializer_space = []

    _SERIALIZERS = [numcodecs.zarr3.PCodec, numcodecs.zarr3.ZFPY]
    if _WITH_EBCC:
        _SERIALIZERS.append(EBCCZarrFilter)
    if _WITH_NUMCODECS_WASM:
        _SERIALIZERS.append(Zfp)

    _SERIALIZER_MAP = {cls.__name__.lower(): cls for cls in _SERIALIZERS}
    if serializer_class and serializer_class.lower() in _SERIALIZER_MAP:
        _SERIALIZERS = [_SERIALIZER_MAP[serializer_class.lower()]]
    elif serializer_class and serializer_class.lower() == "none":
        _SERIALIZERS = []
        serializer_space.append(None)

    for serializer in _SERIALIZERS:
        if serializer == numcodecs.zarr3.PCodec:
            for level in [0, 4, 8, 12]:
                for delta_encoding_order in [0, 3, 7]:
                    serializer_space.append(serializer(
                            level=level,
                            mode_spec="auto",
                            delta_spec="auto",
                            delta_encoding_order=delta_encoding_order
                        )
                    )
        elif serializer in (numcodecs.zarr3.ZFPY, Zfp):
            # https://github.com/LLNL/zfp/tree/develop/python
            # https://github.com/LLNL/zfp/blob/develop/tests/python/test_numpy.py
            _ZFP_MODES = [
                ("fixed-accuracy",  zfpy.mode_fixed_accuracy,  "tolerance", compute_fixed_accuracy_param),
                ("fixed-precision", zfpy.mode_fixed_precision, "precision", compute_fixed_precision_param),
                ("fixed-rate",      zfpy.mode_fixed_rate,      "rate",      compute_fixed_rate_param),
            ]
            if is_int:
                _ZFP_MODES = [m for m in _ZFP_MODES if m[0] == "fixed-rate"]
            for mode_str, zfpy_mode, param_name, param_fn in _ZFP_MODES:
                for k in range(3):
                    val = param_fn(k)
                    if serializer is numcodecs.zarr3.ZFPY:
                        serializer_space.append(serializer(mode=zfpy_mode, **{param_name: val}))
                    else:
                        codec = serializer(mode=mode_str, **{param_name: val})
                        serializer_space.append(AnyNumcodecsArrayBytesCodec(codec))
        elif serializer == EBCCZarrFilter:
            # https://github.com/spcl/ebcc
            data = da.squeeze()  # TODO: add more checks on the shape of the data
            for atol in [1e-2, 1e-3, 1e-6, 1e-9]:
                ebcc_filter = EBCC_Filter(
                        base_cr=2, 
                        height=data.shape[0],
                        width=data.shape[1],
                        residual_opt=("max_error_target", atol)
                    )
                zarr_filter = serializer(ebcc_filter.hdf_filter_opts)
                serializer_space.append(AnyNumcodecsArrayBytesCodec(zarr_filter))

    return list(zip(range(len(serializer_space)), serializer_space))


def valid_keepbits_for_bitround(xr_dataarray, step=1):
    dtype = xr_dataarray.dtype
    if np.issubdtype(dtype, np.float64):
        return inclusive_range(1, 52, step)  # float64 mantissa is 52 bits
    elif np.issubdtype(dtype, np.float32):
        return inclusive_range(1, 23, step)  # float32 mantissa is 23 bits
    else:
        raise TypeError(f"Unsupported dtype '{dtype}'. BitRound only supports float32 and float64.")


def valid_digits_for_quantize(xr_dataarray, step=1):
    dtype = xr_dataarray.dtype
    if np.issubdtype(dtype, np.float64):
        return inclusive_range(1, 15, step)  # ~15–16 significant digits for float64
    elif np.issubdtype(dtype, np.float32):
        return inclusive_range(1, 7, step)   # ~7 significant digits for float32
    else:
        raise TypeError(f"Unsupported dtype '{dtype}'. Quantize only supports float32 and float64.")


def compute_fixed_precision_param(param: int) -> int:
    # https://github.com/LLNL/zfp/tree/develop/tests/python
    return 1 << (param + 3)

def compute_fixed_rate_param(param: int) -> int:
    # https://github.com/LLNL/zfp/tree/develop/tests/python
    return 1 << (param + 3)

def compute_fixed_accuracy_param(param: int) -> float:
    # https://github.com/LLNL/zfp/tree/develop/tests/python
    return math.ldexp(1.0, -(1 << param))


def inclusive_range(start, end, step=1):
    if step == 0:
        raise ValueError("step must not be zero")

    values = []
    i = start
    if step > 0:
        while i <= end:
            values.append(i)
            i += step
        if values[-1] != end:
            values.append(end)
    else:
        while i >= end:
            values.append(i)
            i += step
        if values[-1] != end:
            values.append(end)

    return values


def validate_percentage(ctx, param, value):
    if value is None:
        return None
    try:
        value = float(value)
    except ValueError:
        raise click.BadParameter("Percentage must be a number.")
    if not (1 <= value <= 99):
        raise click.BadParameter("Percentage must be between 1 and 99.")
    return value


def slice_array(arr: pd.array, indices_ls: list) -> np.ndarray:
    arr_ls = []
    for ind in indices_ls:
        arr_ls.append(arr[[ind]])

    sliced_arr = np.hstack(
        tuple(arr_ls)
    )
    return sliced_arr


def unzip_file(zip_path: str, extract_to: str = None):
    if extract_to is None:
        extract_to = os.path.splitext(zip_path)[0]  # default: same name as zip

    # Remove the extract_to path if it exists
    if os.path.exists(extract_to):
        if os.path.isfile(extract_to):
            os.remove(extract_to)
        else:
            shutil.rmtree(extract_to)

    # Extract zip file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)

    return extract_to


def copy_folder_contents(src_folder: str, dst_folder: str):
    os.makedirs(dst_folder, exist_ok=True)

    for item in os.listdir(src_folder):
        src_path = os.path.join(src_folder, item)
        dst_path = os.path.join(dst_folder, item)

        if os.path.isdir(src_path):
            shutil.copytree(src_path, dst_path, dirs_exist_ok=True)
        else:
            shutil.copy2(src_path, dst_path)


def progress_bar(i, total_configs, print_every=100, bar_width=40):
    rank = MPI.COMM_WORLD.Get_rank()
    if rank != 0:
        return

    percent = (i + 1) / total_configs
    filled = int(bar_width * percent)
    bar = "*" * filled + "-" * (bar_width - filled)
    if int(i + 1) % print_every == 0 or (i + 1) == total_configs:
        click.echo(f"[Rank {rank}] Progress: |{bar}| {percent*100:6.2f}% ({i+1}/{total_configs}) [{total_configs} loops per MPI task]")


# Global registry of all timings
_TIMINGS = defaultdict(list)

class Timer:
    def __init__(self, label):
        self.label = label

    def __enter__(self):
        self.start = time.perf_counter()
        return self

    def __exit__(self, *args):
        duration = time.perf_counter() - self.start
        _TIMINGS[self.label].append(duration)

@atexit.register
def print_profile_summary():
    if not _TIMINGS:
        return

    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    
    if rank != 0:
        return # Only the root process prints the summary

    print("\n=== compress_with_zarr Timing Summary ===")

    # Determine max label width for formatting
    label_width = max(len(label) for label in _TIMINGS.keys())

    header = (
        f"{'Label':<{label_width}} | {'Calls':>5} | {'Avg (s)':>10} | {'Total (s)':>10}"
    )
    print(header)
    print("-" * len(header))

    for label, durations in sorted(_TIMINGS.items()):
        total = sum(durations)
        count = len(durations)
        avg = total / count
        print(f"{label:<{label_width}} | {count:>5} | {avg:>10.6f} | {total:>10.6f}")

    print("=" * len(header))


def compute_linear_width(
    da: xr.DataArray,
    *,
    quantile: float = 0.01,
    skipna: bool = True,
    floor: float | None = None,
    cap: float | None = None,
    compute: bool = False,
) -> float | xr.DataArray:
    """Lazy, Dask-enabled estimate of Asinh.linear_width via small-quantile(|da|)."""
    # mask to finite values (lazy)
    finite = xr.apply_ufunc(np.isfinite, da, dask="parallelized")
    abs_da = xr.apply_ufunc(np.abs, da.where(finite), dask="parallelized")

    # small quantile of |da| (lazy)
    lw = abs_da.quantile(quantile, skipna=skipna)

    # drop the 'quantile' coord that xarray adds
    if "quantile" in lw.dims:
        lw = lw.squeeze("quantile", drop=True)

    # Dask-safe bounds without apply_ufunc
    if floor is not None or cap is not None:
        lw = lw.clip(min=floor if floor is not None else None,
                     max=cap if cap is not None else None)

    return float(lw.compute()) if compute else lw
